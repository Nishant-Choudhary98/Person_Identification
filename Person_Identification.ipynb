{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Person Identification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOhVIkYseH56Fm4ZS3AhrSr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dd07919a4155401b97abdefbc0364fe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_25decd8d29ba4074a45fefb32a481f13",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_efbc8b4b41814158ba9cfc67996092a3",
              "IPY_MODEL_4be965acf5ee484391d7d2f412eb3c83"
            ]
          }
        },
        "25decd8d29ba4074a45fefb32a481f13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "efbc8b4b41814158ba9cfc67996092a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4a866da59de64634bae24a7ea92de28f",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 167502836,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 167502836,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b1d13f06c93e4733b1f1647abd234572"
          }
        },
        "4be965acf5ee484391d7d2f412eb3c83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_091cb8c934e140c991d54edf3489cd51",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 160M/160M [00:01&lt;00:00, 126MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a2fe6912b200483ea57b1ce0ff224d97"
          }
        },
        "4a866da59de64634bae24a7ea92de28f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b1d13f06c93e4733b1f1647abd234572": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "091cb8c934e140c991d54edf3489cd51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a2fe6912b200483ea57b1ce0ff224d97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nishant-Choudhary98/Person_Identification/blob/main/Person_Identification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhYa5ORdqJCX",
        "outputId": "0d229103-fac4-40e5-b0da-5ea24c5f2faf"
      },
      "source": [
        "\n",
        "import torch\n",
        "cuda_enable = True\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda:0\")\n",
        "  print(\"GPU\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print(\"CPU\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gbh2WnW9qNS6"
      },
      "source": [
        "import os\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torch import nn\n",
        "from pycocotools.coco import COCO\n",
        "from torchvision import datasets, transforms, models"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mIC6IocqPz-",
        "outputId": "ee7d11a5-73ed-4a30-c10c-76665b9a0a7f"
      },
      "source": [
        "!git clone https://github.com/Chang-Chia-Chi/Pedestrian.git\n",
        "!git clone https://github.com/Chang-Chia-Chi/Final-Project.git"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Pedestrian' already exists and is not an empty directory.\n",
            "fatal: destination path 'Final-Project' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjeeaDIDqT1d"
      },
      "source": [
        "class ImageDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, root, annotation, transforms=None):\n",
        "    self.root = root\n",
        "    self.transforms = transforms\n",
        "    self.coco = COCO(annotation)\n",
        "    self.ids = list(sorted(self.coco.imgs.keys()))\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # image ID\n",
        "    img_id = self.ids[idx]\n",
        "    # image file_name\n",
        "    img_file = self.coco.loadImgs(img_id)[0][\"file_name\"]\n",
        "    # read_imgae\n",
        "    img = Image.open(os.path.join(self.root, img_file))\n",
        "    # get annotation ID\n",
        "    ann_ids = self.coco.getAnnIds(imgIds = img_id)\n",
        "    # read annotation\n",
        "    anns = self.coco.loadAnns(ann_ids)\n",
        "    # num of people in the picture\n",
        "    num_objs = len(anns)\n",
        "    # build information about bounding box & area\n",
        "    boxes = []\n",
        "    areas = []\n",
        "    for i in range(num_objs):\n",
        "      x_min = anns[i]['bbox'][0]\n",
        "      y_min = anns[i]['bbox'][1]\n",
        "      x_max = x_min + anns[i]['bbox'][2]\n",
        "      y_max = y_min + anns[i]['bbox'][3]\n",
        "      boxes.append([x_min, y_min, x_max, y_max])\n",
        "      areas.append(anns[i]['area'])\n",
        "  \n",
        "    # transfer information to Tensor\n",
        "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "    labels = torch.ones((num_objs,), dtype = torch.int64)\n",
        "    img_id = torch.tensor([img_id])\n",
        "    areas = torch.as_tensor(areas, dtype=torch.float32)\n",
        "    iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "    # Annotation in dict form\n",
        "    Annotations = {\n",
        "        \"boxes\" : boxes,\n",
        "        \"labels\" : labels,\n",
        "        \"image_id\" : img_id,\n",
        "        \"area\" : areas,\n",
        "        \"iscrowd\" : iscrowd\n",
        "    }\n",
        "\n",
        "    # transforms\n",
        "    if self.transforms is not None:\n",
        "      img = self.transforms(img)\n",
        "    \n",
        "    return img, Annotations\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.ids)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8w2hxeqRvI2K",
        "outputId": "af2e0e75-337e-4221-bf8b-b844e9db6c70"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48QJna6FqYXG",
        "outputId": "e303b43c-86cf-4056-9a85-2834a702c7a6"
      },
      "source": [
        "def get_transforms(train):\n",
        "  trans = []\n",
        "  if train:\n",
        "    trans.append(transforms.RandomHorizontalFlip(0.5))\n",
        "    # trans.append(transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2)))\n",
        "    # trans.append(transforms.ColorJitter(brightness=1, contrast=1, saturation=1))\n",
        "  trans.append(transforms.ToTensor())\n",
        "  return transforms.Compose(trans)\n",
        "\n",
        "train_data_path = '/content/Pedestrian/PNGImages'\n",
        "coco_path = '/content/Pedestrian/Json/train.json'\n",
        "train_dataset = ImageDataset(root=train_data_path, annotation=coco_path, transforms=get_transforms(train=True))\n",
        "\n",
        "# collate_fn needs for batch\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, 2, shuffle=True, collate_fn=collate_fn)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102,
          "referenced_widgets": [
            "dd07919a4155401b97abdefbc0364fe5",
            "25decd8d29ba4074a45fefb32a481f13",
            "efbc8b4b41814158ba9cfc67996092a3",
            "4be965acf5ee484391d7d2f412eb3c83",
            "4a866da59de64634bae24a7ea92de28f",
            "b1d13f06c93e4733b1f1647abd234572",
            "091cb8c934e140c991d54edf3489cd51",
            "a2fe6912b200483ea57b1ce0ff224d97"
          ]
        },
        "id": "IwMGPWafqa30",
        "outputId": "d8547a1d-9f6b-4c88-a2f1-6361678e6098"
      },
      "source": [
        "model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd07919a4155401b97abdefbc0364fe5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=167502836.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8fcu12jqc_v"
      },
      "source": [
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "# change question to binary classification (human, not human)\n",
        "num_classes = 2\n",
        "\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNdX_GVfrMf9",
        "outputId": "d751af1c-2076-409a-ad38-59d9bb54c334"
      },
      "source": [
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005,\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                step_size=3,\n",
        "                                                gamma=0.1)\n",
        "\n",
        "epochs = 5\n",
        "model = model.to(device)\n",
        "\n",
        "for _ in range(epochs):\n",
        "  model.train()\n",
        "  i = 0\n",
        "  for imgs, annotations in train_loader:\n",
        "    i += 1\n",
        "    imgs = list(img.to(device) for img in imgs)\n",
        "    annotations = [{k:v.to(device) for k, v in t.items()} for t in annotations]\n",
        "    loss_dict = model(imgs, annotations)\n",
        "    losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    losses.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(\"Iteration: {}; Loss: {}\".format(i, losses))\n",
        "  lr_scheduler.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 1; Loss: 1.9277527332305908\n",
            "Iteration: 2; Loss: 0.8038103580474854\n",
            "Iteration: 3; Loss: 1.0359573364257812\n",
            "Iteration: 4; Loss: 0.29589542746543884\n",
            "Iteration: 5; Loss: 0.9685311913490295\n",
            "Iteration: 6; Loss: 1.547301173210144\n",
            "Iteration: 7; Loss: 1.0242676734924316\n",
            "Iteration: 8; Loss: 0.569017767906189\n",
            "Iteration: 9; Loss: 0.5179170370101929\n",
            "Iteration: 10; Loss: 0.8419072031974792\n",
            "Iteration: 11; Loss: 1.1414035558700562\n",
            "Iteration: 12; Loss: 0.28136831521987915\n",
            "Iteration: 13; Loss: 0.4242938160896301\n",
            "Iteration: 14; Loss: 1.353951334953308\n",
            "Iteration: 15; Loss: 1.0222477912902832\n",
            "Iteration: 16; Loss: 1.3635865449905396\n",
            "Iteration: 17; Loss: 0.9115835428237915\n",
            "Iteration: 18; Loss: 0.4018421173095703\n",
            "Iteration: 19; Loss: 0.3855057656764984\n",
            "Iteration: 20; Loss: 0.3262733817100525\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKz07xQtrPNt"
      },
      "source": [
        "def im_convert(tensor):\n",
        "    image = tensor.clone().detach().cpu().numpy()\n",
        "    \n",
        "    # [C, W, H] --> [W, H, C]\n",
        "    image = image.transpose(1, 2, 0)\n",
        "\n",
        "    # denormalize\n",
        "    image = image * np.array((0.5, 0.5, 0.5)) + np.array((0.5, 0.5, 0.5))\n",
        "\n",
        "    \n",
        "    image = image.clip(0, 1)\n",
        "    \n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FuNUo04rj8-"
      },
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# use 10.pic to illustrate performance for a single picture.\n",
        "img = cv2.imread(\"/content/Final-Project/10.jpg\")\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# transfer array to pillow format\n",
        "val_tran = get_transforms(train=False)\n",
        "im_pil = Image.fromarray(img)\n",
        "im_pil = val_tran(im_pil)\n",
        "plt.imshow(im_convert(im_pil))\n",
        "\n",
        "image = im_pil.to(device)\n",
        "model_val = model.eval()\n",
        "output = model([image])\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2RZiZodrqAb"
      },
      "source": [
        "scores = output[0]['scores'].detach().cpu().numpy()\n",
        "num_people = len(scores[scores > 0.7])\n",
        "\n",
        "boxes = output[0]['boxes'].detach().cpu().numpy()\n",
        "boxes = boxes[:num_people]\n",
        "\n",
        "for i in range(num_people):\n",
        "  img = cv2.rectangle(img, (boxes[i][0], boxes[i][1]), (boxes[i][2], boxes[i][3]), (0, 255, 0), thickness=10)\n",
        "  img = cv2.putText(img, \"score: {:.3f}\".format(scores[i]), (boxes[i][0], int(boxes[i][1] - 100)),\n",
        "                    cv2.FONT_HERSHEY_PLAIN, 4, (0, 255, 0), thickness=5)\n",
        "\n",
        "img = cv2.putText(img,\"Number of people: {}\".format(num_people), (50, 200),\n",
        "                  cv2.FONT_HERSHEY_COMPLEX, 8, (255, 0, 0), thickness=15)\n",
        "\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "cv2.imwrite('output.jpg', img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKX8WfLErxx_"
      },
      "source": [
        "cap = cv2.VideoCapture(\"/content/Final-Project/final.mp4\")\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "\n",
        "out = cv2.VideoWriter('output.avi', fourcc, 30.0, (1920, 1080))\n",
        "i = 1\n",
        "while cap.isOpened():\n",
        "  print(i)\n",
        "  ret, frame = cap.read()\n",
        "  if ret:\n",
        "    if i < 152:\n",
        "      out.write(frame)\n",
        "      i += 1\n",
        "      continue\n",
        "    elif i > 451 and i < 512:\n",
        "      out.write(frame)\n",
        "      i += 1\n",
        "      continue     \n",
        "    elif i > 748 and i < 809:\n",
        "      out.write(frame)\n",
        "      i += 1\n",
        "      continue   \n",
        "    elif i > 1078 and i < 1139:\n",
        "      out.write(frame)\n",
        "      i += 1\n",
        "      continue   \n",
        "    elif i > 1378 and i < 1439:\n",
        "      out.write(frame)\n",
        "      i += 1\n",
        "      continue   \n",
        "    elif i > 1661 and i < 1722:\n",
        "      out.write(frame)\n",
        "      i += 1\n",
        "      continue   \n",
        "    else:\n",
        "      frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "      val_tran = get_transforms(train=False)\n",
        "      im_pil = Image.fromarray(frame)\n",
        "      im_pil = val_tran(im_pil)\n",
        "\n",
        "      image = im_pil.to(device)\n",
        "      model_val = model.eval()\n",
        "      output = model([image])\n",
        "      scores = output[0]['scores'].detach().cpu().numpy()\n",
        "      num_people = len(scores[scores > 0.6])\n",
        "\n",
        "      boxes = output[0]['boxes'].detach().cpu().numpy()\n",
        "      boxes = boxes[:num_people]\n",
        "\n",
        "      for j in range(num_people):\n",
        "        frame = cv2.rectangle(frame, (boxes[j][0], boxes[j][1]), (boxes[j][2], boxes[j][3]), (0, 255, 0), thickness=10)\n",
        "        frame = cv2.putText(frame, \"score: {:.3f}\".format(scores[j]), (boxes[j][0], int(boxes[j][1] - 50)),\n",
        "                          cv2.FONT_HERSHEY_PLAIN, 3, (0, 255, 0), thickness=5)\n",
        "\n",
        "      frame = cv2.putText(frame,\"Number of people: {}\".format(num_people), (50, 100),\n",
        "                        cv2.FONT_HERSHEY_COMPLEX, 5, (255, 0, 0), thickness=15)\n",
        "      \n",
        "      frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "      out.write(frame)\n",
        "      i += 1 \n",
        "\n",
        "  if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "    break\n",
        "  if i > 2021 :\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}